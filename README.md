# Лабораторная работа по курсу "Искусственный интеллект"
# Многослойный персептрон

| Студент | Ли Алиса И. |
|---------|-------------|
| Группа  | М8О-308б    |
| Оценка 1 (свой фреймворк) | |
| Оценка 2 (PyTorch/Tensorflow) | |
| Проверил | 

> *Комментарии проверяющего*
### Задание

Решить задачу классификации для датасетов MNIST, FashionMNIST, CIFAR-10 с помощью 1, 2 и 3-слойного персептрона. Попробовать разные передаточные функции, провести сравнительную оценку решений. Решение сделать двумя способами:
* "С нуля", на основе базовых операций библиотеки numpy. Решение желательно реализовать в виде библиотеки, пригодной для решения более широкго круга задач.
* На основе одного из существующих нейросетевых фреймворков, в соответствии с вариантом задания:
   1. PyTorch
   1. Tensorflow/Keras

> *Номер варианта вычисляется по формуле 1 + (N-1) mod 2, где N - номер студента в списке.*

Решение оформить в файлах [Solution_MyFramework.ipynb](Solution_MyFramework.ipynb) и [Solution.ipynb](Solution.ipynb). 
Отчет по работе и сравнение методов пишется в этом файле после задания.
### Критерии оценки

Первая часть лабораторной работы:

| Сделано | Баллы |
|---------|-------|
| Реализован однослойный персептрон, классифицирующий датасет с точностью >85% |  |
| Реализован многослойный персептрон, классифицирующий датасет с точностью >85% |  |
| Реализация сделана как библиотека для обучения сетей различных конфигураций, в соответствии с примером |  |
| Улучшена архитектура библиотеки, отдельно вынесены алгоритмы обучения, функции потерь |  |
| Проведено сравнение различных гиперпараметров, таких, как передаточные функции, число нейронов в промежуточных слоях, функции потерь, с графиками обучения и матрицами неточности |  |
| Проведен анализ для датасета FashionMNIST | |

Вторая часть лабораторной работы:

| Сделано | Баллы |
|---------|-------|
| Реализован однослойный персептрон, классифицирующий датасет с точностью >85% |  |
| Реализован многослойный персептрон, классифицирующий датасет с точностью >85% |  |
| Реализация использует возможности базового фреймворка, включая работу с данными |  |
| Проведено сравнение различных гиперпараметров, таких, как передаточные функции, число нейронов в промежуточных слоях, функции потерь, с графиками обучения и матрицами неточности |  |
| Проведен анализ для датасета FashionMNIST |  |
| Проведен анализ для другого датасета с цветными картинками (CIFAR-10) |  |

## Отчёт по работе

Отчёт, помимо общего описания решения, должен включать:
* График точности на обучающей и проверочной выборке в процессе обучения
* Confusion Matrix
* Сравнение полученных показателей точности модели для различных гиперпараметров (передаточных функций, числа нейронов в слоях и т.д.)

## Материалы для изучения

 * [Реализация своего нейросетевого фреймворка](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroMyFw.ipynb)
 * [Введение в PyTorch](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroPyTorch.ipynb)
 * [Введение в Tensorflow/Keras](https://github.com/shwars/NeuroWorkshop/blob/master/Notebooks/IntroKerasTF.ipynb)
 
 Принцип работы: 
 Был реализован персептрон. Его задачей являлась классификация объектов из датасетов. К нашим входным данным мы применяем линейную функцию. Полученный результат при помощи функции SoftMax преобразуем в набор вероятностей, соответствующий набору классов. Считаем функцию ошибки и будем минимизировать ее для получения максимальной точности. Чтобы подогнать аргументы, мы используем обратное распространение ошибки и градиентный спуск. На каждом шаге корректируем аргументы в соответствии с подсчитанными производными. Для работы перспетрона были использованы библиотечные датасеты MNIST, FashionMNIST, CIFAR-10. При реализации на pyTorch нам не пришлось считать производные вручную, они были подсчитаны при помощи внутреннего функционала фреймворка.
 Выводы: многослойный персептрон является хорошим классификатором для простых датасетов вроде MNIST или FasionMNIST. Он довольно быстр и прост в реализации. Однако на такаих данных как например cifar10 данная сеть даже реализованная с помощью фреймворка torch не справляется с задачей. На таких данных стоит использовать более сложные нейронные сети
